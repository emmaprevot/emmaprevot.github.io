---
title: "Bayesian Statistics"
collection: teaching
type: "Doctoral training course"
permalink: /teaching/2014-spring-teaching-1
venue: "University of Oxford, Big Data Institute"
date: 2025-01-20
location: "Oxford, UK"
---

This module explores Bayesian statistics, offering a distinct paradigm from classical inference. Bayesian methods underpin many modern machine learning techniques and are essential for tasks like probabilistic modeling and uncertainty quantification. Key topics covered include Bayesian parameter estimation, Monte Carlo methods, hierarchical modeling, latent variable analysis, tree-based learning, and causal inference. In the practical sessions, which I tutored, students used R notebooks to implement these concepts, gaining hands-on experience with tools such as MCMC algorithms, hierarchical Bayesian models, dimensionality reduction methods, and causal analysis techniques.

Here is a brief summary of the course content covered in the lectures and was then covered in the coding practicals:

Lecture 1: Introduction to Bayesian Statistics​
======

Overview of Bayesian inference, including prior, likelihood, posterior, and Bayes' rule. Motivation for probabilistic modeling in machine learning. Parameter estimation methods (exact, Monte Carlo, variational). Advantages of Bayesian approaches, such as credible intervals and sequential updating.

Lecture 2: Monte Carlo Methods​
======

Importance of Monte Carlo techniques in scenarios with intractable posteriors. Sampling methods: rejection sampling, importance sampling.
Introduction to Markov Chain Monte Carlo (MCMC), including Metropolis-Hastings and Gibbs sampling. Discussion on burn-in, mixing time, and convergence diagnostics.

Lecture 3: Hierarchical Models and Priors​
======

Explanation of priors (informative vs. uninformative) and their influence. Hierarchical Bayesian modeling for data with multi-level structures. Approximation methods like Laplace and Expectation Maximization (EM). Applications such as Bayesian Lasso and factor analysis.

Lecture 4: Probabilistic Latent Variable Models​
======

Focus on dimensionality reduction techniques, including PCA, Factor Analysis (FA), and ICA. Introduction to hierarchical and generative models for high-dimensional data. Identification and sparsity-inducing priors to improve interpretability.

Lecture 5: Tree-Based Learning​
======

Overview of tree-based methods: decision trees, random forests, and Bayesian Additive Regression Trees (BART). Use cases in regression, classification, and feature interaction modeling. Comparative insights: flexibility of BART over linear regression.

Lecture 6: Causal Inference in Observational Studies​
======
Principles of causal inference, including counterfactual reasoning. Challenges like confounding and bias. Methods: stratification, matching, propensity scores, and instrumental variables. Use of directed acyclic graphs (DAGs) for causal modeling.


